# Story 4.5: Avatar Lip-Sync and Emotional Expressions

**Epic:** Epic 4 - Video Calling Integration  
**Status:** Draft  
**Created:** December 20, 2025  
**Last Updated:** December 20, 2025

---

## Story

**As a** student user,  
**I want** the avatar's mouth to sync with speech and show natural facial expressions,  
**so that** the counseling session feels realistic and engaging.

---

## Acceptance Criteria

1. Avatar lip-sync accuracy must be 95%+ (synchronized with audio within 50ms latency).
2. Avatar displays appropriate emotional expressions during conversation: supportive (nodding, gentle smile), concerned (furrowed brow), encouraging (bright smile, open posture).
3. Emotional expressions triggered by conversation context (e.g., crisis keywords trigger concerned expression).
4. Transitions between expressions must be smooth (no sudden jumps), taking 300-500ms.
5. Avatar maintains natural eye contact with user (looking at camera 80% of time, occasional glances away for natural behavior).
6. If video quality degrades (low bandwidth), avatar automatically reduces animation complexity to maintain frame rate.
7. Beyond Presence avatar configuration includes custom emotional presets for counseling scenarios.
8. Avatar animation tested with high-quality video output (720p minimum, 30fps).
9. Lip-sync remains accurate even with network jitter or packet loss up to 5%.

---

## Tasks / Subtasks

- [ ] Configure Beyond Presence avatar lip-sync settings (AC: 1, 9)
  - [ ] Enable high-accuracy lip-sync mode
  - [ ] Set audio-visual sync threshold to 50ms
  - [ ] Configure phoneme mapping for English
  - [ ] Test with various speech speeds
  - [ ] Validate accuracy with automated tests
  
- [ ] Define counseling emotional expression presets (AC: 2, 3)
  - [ ] Create "supportive" expression (nodding, gentle smile, warm gaze)
  - [ ] Create "concerned" expression (furrowed brow, attentive posture)
  - [ ] Create "encouraging" expression (bright smile, open body language)
  - [ ] Create "neutral listening" expression (baseline)
  - [ ] Map expressions to conversation contexts
  
- [ ] Implement context-aware expression triggering (AC: 3)
  - [ ] Integrate with OpenAI Realtime Model sentiment analysis
  - [ ] Detect crisis keywords (e.g., "suicidal", "hurt myself")
  - [ ] Trigger concerned expression for crisis content
  - [ ] Trigger encouraging expression for positive progress
  - [ ] Maintain supportive expression as default
  
- [ ] Ensure smooth expression transitions (AC: 4)
  - [ ] Configure transition duration (300-500ms)
  - [ ] Use easing functions for natural movement
  - [ ] Prevent rapid expression switching
  - [ ] Test transition quality visually
  
- [ ] Configure natural eye contact behavior (AC: 5)
  - [ ] Primary gaze direction: camera (80% of time)
  - [ ] Occasional glances away (10-15 degree angle)
  - [ ] Blink rate: 15-20 times per minute
  - [ ] Avoid "staring" effect with micro-movements
  
- [ ] Implement adaptive quality management (AC: 6)
  - [ ] Monitor video bitrate and frame rate
  - [ ] Detect low bandwidth conditions
  - [ ] Reduce animation complexity when needed
  - [ ] Prioritize lip-sync over secondary animations
  - [ ] Log quality degradation events
  
- [ ] Test avatar video quality (AC: 8)
  - [ ] Verify 720p resolution output
  - [ ] Verify 30fps frame rate
  - [ ] Test on different network conditions
  - [ ] Validate with multiple avatar IDs
  - [ ] Check visual artifacts or glitches
  
- [ ] Test lip-sync under degraded network (AC: 9)
  - [ ] Simulate packet loss (1%, 3%, 5%)
  - [ ] Simulate network jitter (10ms, 50ms, 100ms)
  - [ ] Verify lip-sync remains acceptable
  - [ ] Document acceptable degradation thresholds
  
- [ ] Document expression configuration
  - [ ] Create expression preset JSON
  - [ ] Document trigger conditions
  - [ ] Provide customization guide
  - [ ] Include troubleshooting tips

---

## Dev Notes

### Architecture Overview

**Beyond Presence Avatar System:**
- **Lip-Sync Engine:** Real-time audio-to-viseme mapping
- **Expression System:** Emotional presets with blending
- **Animation Pipeline:** Smooth transitions with easing
- **Quality Adaptation:** Dynamic complexity reduction

**Integration with OpenAI Realtime Model:**
- OpenAI generates audio + text
- Audio sent to Beyond Presence for lip-sync
- Text analyzed for sentiment/context
- Appropriate expression triggered

### Avatar Configuration with Expressions

**avatar_agent/avatar_config.py:**
```python
from enum import Enum
from typing import Dict, Any

class EmotionalExpression(Enum):
    """Counseling emotional expressions"""
    SUPPORTIVE = "supportive"
    CONCERNED = "concerned"
    ENCOURAGING = "encouraging"
    NEUTRAL_LISTENING = "neutral_listening"

# Emotional expression presets for Beyond Presence
EXPRESSION_PRESETS: Dict[EmotionalExpression, Dict[str, Any]] = {
    EmotionalExpression.SUPPORTIVE: {
        "name": "Supportive",
        "facial_config": {
            "smile_intensity": 0.5,  # Gentle smile
            "eye_openness": 0.75,    # Warm, open eyes
            "eyebrow_position": 0.2, # Slightly raised (receptive)
            "head_tilt": 3,           # Slight tilt (5 degrees) - empathetic
        },
        "body_language": {
            "posture": "open",
            "lean_forward": 2,        # Leaning in slightly
            "hand_gestures": "minimal",
        },
        "animation": {
            "nodding_frequency": 0.15,  # Occasional nodding
            "micro_expressions": True,
        }
    },
    EmotionalExpression.CONCERNED: {
        "name": "Concerned",
        "facial_config": {
            "smile_intensity": 0.0,   # No smile
            "eye_openness": 0.85,     # Very attentive
            "eyebrow_position": -0.3, # Slightly furrowed
            "head_tilt": 0,           # Straight, focused
        },
        "body_language": {
            "posture": "attentive",
            "lean_forward": 5,         # Leaning in more
            "hand_gestures": "minimal",
        },
        "animation": {
            "nodding_frequency": 0.0,   # No nodding
            "micro_expressions": True,
        }
    },
    EmotionalExpression.ENCOURAGING: {
        "name": "Encouraging",
        "facial_config": {
            "smile_intensity": 0.8,    # Bright smile
            "eye_openness": 0.9,       # Wide, engaged eyes
            "eyebrow_position": 0.4,   # Raised (positive)
            "head_tilt": 0,
        },
        "body_language": {
            "posture": "open",
            "lean_forward": 1,
            "hand_gestures": "moderate", # More expressive
        },
        "animation": {
            "nodding_frequency": 0.2,   # Frequent nodding
            "micro_expressions": True,
        }
    },
    EmotionalExpression.NEUTRAL_LISTENING: {
        "name": "Neutral Listening",
        "facial_config": {
            "smile_intensity": 0.2,    # Slight smile
            "eye_openness": 0.7,
            "eyebrow_position": 0.0,   # Neutral
            "head_tilt": 0,
        },
        "body_language": {
            "posture": "neutral",
            "lean_forward": 0,
            "hand_gestures": "minimal",
        },
        "animation": {
            "nodding_frequency": 0.05,  # Rare nodding
            "micro_expressions": False,
        }
    },
}

# Eye contact configuration
EYE_CONTACT_CONFIG = {
    "primary_gaze": "camera",
    "camera_focus_percentage": 80,  # 80% looking at camera
    "glance_away_angle": 12,        # degrees
    "glance_duration": 1.5,         # seconds
    "blink_rate_per_minute": 18,
}

# Lip-sync configuration
LIP_SYNC_CONFIG = {
    "accuracy_mode": "high",        # high, medium, low
    "sync_threshold_ms": 50,       # Max 50ms latency
    "phoneme_mapping": "english",
    "audio_sample_rate": 24000,    # Hz
}

# Transition configuration
TRANSITION_CONFIG = {
    "duration_ms": 400,             # 400ms transitions
    "easing": "ease-in-out",
    "min_interval_ms": 3000,        # Min 3s between expression changes
}

# Quality adaptation
QUALITY_ADAPTATION_CONFIG = {
    "bitrate_threshold_low": 500,   # kbps
    "fps_threshold_low": 20,
    "reduce_secondary_animations": True,
    "prioritize_lip_sync": True,
}
```

### Enhanced Video Agent with Expression Control

**avatar_agent/video_agent.py (updated from Story 4.2):**
```python
import asyncio
import logging
from typing import Optional
from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    WorkerOptions,
    cli,
    llm,
)
from beyond_presence import AvatarSession, EmotionalState
import openai
from avatar_config import (
    EXPRESSION_PRESETS,
    EmotionalExpression,
    EYE_CONTACT_CONFIG,
    LIP_SYNC_CONFIG,
    TRANSITION_CONFIG,
    QUALITY_ADAPTATION_CONFIG,
)

logger = logging.getLogger(__name__)

class VideoAvatarAgent:
    """LiveKit agent with Beyond Presence avatar and emotional expressions"""
    
    def __init__(self, ctx: JobContext, avatar_id: str, category: str):
        self.ctx = ctx
        self.avatar_id = avatar_id
        self.category = category
        self.avatar_session: Optional[AvatarSession] = None
        self.current_expression = EmotionalExpression.NEUTRAL_LISTENING
        self.last_expression_change = 0
        
    async def initialize_avatar(self):
        """Initialize Beyond Presence avatar with expression config"""
        logger.info(f"Initializing avatar {self.avatar_id} for {self.category}")
        
        self.avatar_session = AvatarSession(
            avatar_id=self.avatar_id,
            api_key=os.getenv("BEY_AVATAR_API_KEY"),
            # Lip-sync configuration
            lip_sync=LIP_SYNC_CONFIG,
            # Eye contact configuration
            eye_contact=EYE_CONTACT_CONFIG,
            # Quality settings
            video_config={
                "resolution": "720p",
                "fps": 30,
                "bitrate": 2000,  # kbps
            },
            # Enable expression system
            enable_expressions=True,
            expression_presets=EXPRESSION_PRESETS,
            transition_config=TRANSITION_CONFIG,
        )
        
        await self.avatar_session.connect()
        logger.info("Avatar session connected")
        
        # Set initial expression
        await self.set_expression(EmotionalExpression.SUPPORTIVE)
        
    async def set_expression(self, expression: EmotionalExpression):
        """Change avatar emotional expression with smooth transition"""
        if not self.avatar_session:
            logger.warning("Cannot set expression: avatar session not initialized")
            return
            
        # Prevent rapid expression changes
        current_time = asyncio.get_event_loop().time()
        min_interval = TRANSITION_CONFIG["min_interval_ms"] / 1000.0
        
        if current_time - self.last_expression_change < min_interval:
            logger.debug(f"Skipping expression change (too soon): {expression.value}")
            return
            
        logger.info(f"Changing expression: {self.current_expression.value} -> {expression.value}")
        
        preset = EXPRESSION_PRESETS[expression]
        await self.avatar_session.set_expression(
            facial_config=preset["facial_config"],
            body_language=preset["body_language"],
            animation=preset["animation"],
            transition_duration=TRANSITION_CONFIG["duration_ms"]
        )
        
        self.current_expression = expression
        self.last_expression_change = current_time
        
    async def analyze_sentiment_and_express(self, text: str):
        """Analyze text sentiment and trigger appropriate expression"""
        text_lower = text.lower()
        
        # Crisis keywords -> concerned expression
        crisis_keywords = [
            "suicide", "suicidal", "kill myself", "end it all",
            "hurt myself", "self-harm", "don't want to live"
        ]
        if any(keyword in text_lower for keyword in crisis_keywords):
            await self.set_expression(EmotionalExpression.CONCERNED)
            return
            
        # Positive progress keywords -> encouraging expression
        positive_keywords = [
            "better", "improving", "good", "progress", "proud",
            "accomplished", "success", "happy"
        ]
        if any(keyword in text_lower for keyword in positive_keywords):
            await self.set_expression(EmotionalExpression.ENCOURAGING)
            return
            
        # Default: supportive expression
        if self.current_expression != EmotionalExpression.SUPPORTIVE:
            await self.set_expression(EmotionalExpression.SUPPORTIVE)
            
    async def monitor_video_quality(self):
        """Monitor video quality and adapt avatar complexity"""
        while True:
            if not self.avatar_session:
                await asyncio.sleep(5)
                continue
                
            stats = await self.avatar_session.get_stats()
            bitrate = stats.get("bitrate_kbps", 2000)
            fps = stats.get("fps", 30)
            
            # Check if quality is degrading
            if (bitrate < QUALITY_ADAPTATION_CONFIG["bitrate_threshold_low"] or
                fps < QUALITY_ADAPTATION_CONFIG["fps_threshold_low"]):
                
                logger.warning(f"Low video quality detected: {bitrate}kbps, {fps}fps")
                
                # Reduce animation complexity
                if QUALITY_ADAPTATION_CONFIG["reduce_secondary_animations"]:
                    await self.avatar_session.set_animation_quality("low")
                    logger.info("Reduced avatar animation complexity")
                    
            else:
                # Restore full quality
                await self.avatar_session.set_animation_quality("high")
                
            await asyncio.sleep(5)  # Check every 5 seconds
            
    async def process_audio_with_lipsync(self, audio_data: bytes):
        """Process audio through avatar with lip-sync"""
        if not self.avatar_session:
            logger.error("Avatar session not initialized")
            return
            
        # Send audio to Beyond Presence for lip-sync rendering
        await self.avatar_session.speak(
            audio_data=audio_data,
            sample_rate=LIP_SYNC_CONFIG["audio_sample_rate"],
            sync_threshold_ms=LIP_SYNC_CONFIG["sync_threshold_ms"],
        )
        
    async def run(self):
        """Main agent loop"""
        await self.initialize_avatar()
        
        # Start quality monitoring task
        quality_task = asyncio.create_task(self.monitor_video_quality())
        
        # Get system prompt for category
        system_prompt = CATEGORY_PROMPTS.get(self.category, CATEGORY_PROMPTS["General"])
        
        # Initialize OpenAI Realtime Model
        openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Main conversation loop
        room = self.ctx.room
        
        async for participant in room.participants.values():
            if participant.identity == "student":
                # Subscribe to student audio
                async for track_publication in participant.tracks.values():
                    if track_publication.kind == rtc.TrackKind.AUDIO:
                        track = await track_publication.subscribe()
                        
                        # Process student speech
                        async for audio_frame in track:
                            # Send to OpenAI for response
                            response = await openai_client.audio.speech.create(
                                model="gpt-4o-realtime-preview",
                                input=audio_frame,
                                voice="alloy",
                                response_format="pcm",
                            )
                            
                            # Analyze sentiment for expression
                            transcription = await openai_client.audio.transcriptions.create(
                                model="whisper-1",
                                file=audio_frame
                            )
                            await self.analyze_sentiment_and_express(transcription.text)
                            
                            # Render with avatar lip-sync
                            await self.process_audio_with_lipsync(response.audio)
                            
        # Cleanup
        quality_task.cancel()
        await self.avatar_session.disconnect()

async def entrypoint(ctx: JobContext):
    """Agent entrypoint"""
    # Extract category from room metadata
    room_metadata = ctx.room.metadata
    category = room_metadata.get("category", "General")
    avatar_id = AVATAR_CONFIG.get(category, AVATAR_CONFIG["General"])
    
    agent = VideoAvatarAgent(ctx, avatar_id, category)
    await agent.run()

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

### Beyond Presence SDK Mock (for Testing)

**avatar_agent/beyond_presence.py (mock for development):**
```python
import asyncio
import logging
from typing import Dict, Any, Optional
from enum import Enum

logger = logging.getLogger(__name__)

class EmotionalState(Enum):
    """Emotional states for avatar"""
    SUPPORTIVE = "supportive"
    CONCERNED = "concerned"
    ENCOURAGING = "encouraging"
    NEUTRAL = "neutral"

class AvatarSession:
    """Mock Beyond Presence Avatar Session"""
    
    def __init__(
        self,
        avatar_id: str,
        api_key: str,
        lip_sync: Dict[str, Any],
        eye_contact: Dict[str, Any],
        video_config: Dict[str, Any],
        enable_expressions: bool = True,
        expression_presets: Optional[Dict] = None,
        transition_config: Optional[Dict] = None,
    ):
        self.avatar_id = avatar_id
        self.api_key = api_key
        self.lip_sync = lip_sync
        self.eye_contact = eye_contact
        self.video_config = video_config
        self.enable_expressions = enable_expressions
        self.expression_presets = expression_presets or {}
        self.transition_config = transition_config or {}
        self.connected = False
        self.current_expression = EmotionalState.NEUTRAL
        
    async def connect(self):
        """Connect to Beyond Presence avatar service"""
        logger.info(f"Connecting to avatar {self.avatar_id}...")
        await asyncio.sleep(0.5)  # Simulate connection delay
        self.connected = True
        logger.info("Avatar session connected")
        
    async def set_expression(
        self,
        facial_config: Dict[str, Any],
        body_language: Dict[str, Any],
        animation: Dict[str, Any],
        transition_duration: int = 400
    ):
        """Set avatar emotional expression"""
        if not self.connected:
            raise RuntimeError("Avatar session not connected")
            
        logger.info(f"Setting expression: {facial_config}")
        await asyncio.sleep(transition_duration / 1000.0)  # Simulate transition
        
    async def speak(
        self,
        audio_data: bytes,
        sample_rate: int = 24000,
        sync_threshold_ms: int = 50
    ):
        """Render avatar speech with lip-sync"""
        if not self.connected:
            raise RuntimeError("Avatar session not connected")
            
        # Simulate lip-sync processing
        duration = len(audio_data) / (sample_rate * 2)  # Assume 16-bit PCM
        logger.info(f"Avatar speaking for {duration:.2f}s with lip-sync")
        await asyncio.sleep(duration)
        
    async def get_stats(self) -> Dict[str, Any]:
        """Get video statistics"""
        # Mock stats
        return {
            "bitrate_kbps": 1500,
            "fps": 30,
            "resolution": "720p",
            "packet_loss": 0.02,
        }
        
    async def set_animation_quality(self, quality: str):
        """Set animation quality (high, medium, low)"""
        logger.info(f"Setting animation quality to: {quality}")
        
    async def disconnect(self):
        """Disconnect avatar session"""
        logger.info("Disconnecting avatar session")
        self.connected = False
```

### Source Tree Updates

```
packages/backend/
└── avatar_agent/
    ├── video_agent.py           # Updated with expression control
    ├── avatar_config.py         # Expression presets and configs
    └── beyond_presence.py       # Mock SDK for testing
```

---

## Testing

### Testing Requirements:

1. **Lip-Sync Accuracy Test:**
   ```python
   import pytest
   import asyncio
   from avatar_agent.video_agent import VideoAvatarAgent
   from avatar_agent.beyond_presence import AvatarSession

   @pytest.mark.asyncio
   async def test_lipsync_accuracy():
       """Test lip-sync accuracy within 50ms threshold"""
       avatar_session = AvatarSession(
           avatar_id="test-avatar",
           api_key="test-key",
           lip_sync={"sync_threshold_ms": 50},
           eye_contact={},
           video_config={}
       )
       
       await avatar_session.connect()
       
       # Simulate audio data
       audio_data = b"\x00" * 24000  # 1 second of audio at 24kHz
       
       start_time = asyncio.get_event_loop().time()
       await avatar_session.speak(audio_data, sample_rate=24000)
       end_time = asyncio.get_event_loop().time()
       
       # Verify timing
       expected_duration = 1.0  # 1 second
       actual_duration = end_time - start_time
       latency_ms = abs(actual_duration - expected_duration) * 1000
       
       assert latency_ms < 50, f"Lip-sync latency {latency_ms}ms exceeds threshold"
   ```

2. **Expression Transition Test:**
   ```python
   @pytest.mark.asyncio
   async def test_expression_transitions():
       """Test smooth expression transitions"""
       from avatar_agent.avatar_config import EmotionalExpression, TRANSITION_CONFIG
       
       agent = VideoAvatarAgent(mock_context, "avatar-id", "Health")
       await agent.initialize_avatar()
       
       # Change expression
       start_time = asyncio.get_event_loop().time()
       await agent.set_expression(EmotionalExpression.ENCOURAGING)
       end_time = asyncio.get_event_loop().time()
       
       # Verify transition duration
       transition_ms = (end_time - start_time) * 1000
       expected_duration = TRANSITION_CONFIG["duration_ms"]
       
       assert abs(transition_ms - expected_duration) < 100, "Transition duration incorrect"
       assert agent.current_expression == EmotionalExpression.ENCOURAGING
   ```

3. **Sentiment Analysis Test:**
   ```python
   @pytest.mark.asyncio
   async def test_sentiment_expression_mapping():
       """Test that sentiment analysis triggers correct expressions"""
       agent = VideoAvatarAgent(mock_context, "avatar-id", "Health")
       await agent.initialize_avatar()
       
       # Crisis text -> concerned expression
       await agent.analyze_sentiment_and_express("I'm feeling suicidal")
       assert agent.current_expression == EmotionalExpression.CONCERNED
       
       # Positive text -> encouraging expression
       await asyncio.sleep(3)  # Wait for min interval
       await agent.analyze_sentiment_and_express("I'm feeling much better today!")
       assert agent.current_expression == EmotionalExpression.ENCOURAGING
   ```

4. **Quality Adaptation Test:**
   ```python
   @pytest.mark.asyncio
   async def test_quality_adaptation():
       """Test avatar adapts to low bandwidth"""
       agent = VideoAvatarAgent(mock_context, "avatar-id", "Health")
       await agent.initialize_avatar()
       
       # Mock low bandwidth stats
       agent.avatar_session.get_stats = lambda: {
           "bitrate_kbps": 400,  # Below 500 threshold
           "fps": 18,            # Below 20 threshold
       }
       
       # Run quality monitoring
       await agent.monitor_video_quality()
       
       # Verify quality was reduced
       # (Would check actual API call in real implementation)
   ```

5. **Manual Testing Checklist:**
   - [ ] Avatar mouth moves in sync with speech
   - [ ] Lip-sync latency imperceptible (<50ms)
   - [ ] Supportive expression during normal conversation
   - [ ] Concerned expression triggered by crisis keywords
   - [ ] Encouraging expression triggered by positive content
   - [ ] Transitions between expressions are smooth (no jumps)
   - [ ] Avatar maintains eye contact (looks at camera)
   - [ ] Occasional natural glances away
   - [ ] Natural blinking (15-20 per minute)
   - [ ] Video quality 720p, 30fps
   - [ ] Lip-sync accurate under 5% packet loss
   - [ ] Animation complexity reduces under low bandwidth
   - [ ] Avatar appears natural and engaging
   - [ ] No visual artifacts or glitches

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-20 | 1.0 | Initial story creation | Sarah (PO) |
