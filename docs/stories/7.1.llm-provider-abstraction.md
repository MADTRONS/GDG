# Story 7.1: Universal LLM Provider Abstraction - Single Provider for All Agents

## Status
**Draft**

---

## Story

**As a** Platform Administrator,  
**I want** to configure one LLM provider (Groq or Gemini) that powers all counselor agents across the entire system,  
**so that** I can easily switch between providers by changing one configuration setting without modifying any code.

---

## Acceptance Criteria

### Functional Requirements

1. Create abstract base class `LLMProvider` with method: `generate(prompt: str, system_message: str, temperature: float, max_tokens: int) -> LLMResponse`
2. `LLMResponse` dataclass includes: `content: str`, `provider_name: str`, `tokens_used: int`, `latency_ms: float`
3. Implement `GroqAdapter` using official `groq` Python SDK
4. Implement `GeminiAdapter` using official `google-generativeai` Python SDK
5. Create `ProviderFactory` that reads `LLM_PROVIDER` environment variable and returns single provider instance for entire system

### Configuration - Single Provider

6. Environment variable `LLM_PROVIDER` accepts values: `"groq"` or `"gemini"` (default: `"gemini"`)
7. Environment variable `GROQ_API_KEY` for Groq configuration
8. Environment variable `GEMINI_API_KEY` for Gemini configuration
9. System uses ONLY the provider specified in `LLM_PROVIDER` across all counselor agents

### Universal Integration - All Counselor Agents

10. **All 6 counselor categories** (Health, Career, Academic, Financial, Social, Personal Development) use the same provider instance
11. Works with **PipeCat voice sessions** - provider generates text responses for all voice calls
12. Works with **Beyond Presence video sessions** - provider generates text responses for all video calls
13. Works with **crisis detection system** - same provider used for keyword detection
14. All counselor agents can run **concurrently** using the same provider without conflicts

### Switching Providers

15. Changing `LLM_PROVIDER` from `"gemini"` to `"groq"` switches entire system without code changes
16. System logs which provider is active on startup
17. Invalid provider name falls back to default (Gemini) with warning log

### Quality Requirements

18. Unit tests for both Groq and Gemini adapters with mocked SDK calls
19. Integration test: All 6 counselor categories generate responses successfully with Groq
20. Integration test: All 6 counselor categories generate responses successfully with Gemini
21. Concurrent session test: 10+ simultaneous conversations using same provider (no resource conflicts)
22. Provider abstraction adds <50ms latency overhead

---

## Tasks / Subtasks

### Phase 1: Create Abstraction Layer
- [ ] **Task 1: Define provider interface** (AC: 1, 2)
  - [ ] Create `backend/providers/base.py` with abstract `LLMProvider` class
  - [ ] Define `generate()` method signature
  - [ ] Create `LLMResponse` dataclass with required fields
  - [ ] Create exception classes: `ProviderError`, `RateLimitError`, `InvalidKeyError`

### Phase 2: Implement Providers
- [ ] **Task 2: Implement Groq adapter** (AC: 3)
  - [ ] Install `groq` SDK: `pip install groq`
  - [ ] Create `backend/providers/groq_adapter.py`
  - [ ] Implement `GroqAdapter.generate()` method
  - [ ] Normalize Groq response to `LLMResponse` format
  - [ ] Handle errors: rate limits, timeouts, invalid keys
  - [ ] Unit tests with mocked Groq SDK

- [ ] **Task 3: Implement Gemini adapter** (AC: 4)
  - [ ] Install `google-generativeai` SDK
  - [ ] Create `backend/providers/gemini_adapter.py`
  - [ ] Implement `GeminiAdapter.generate()` method
  - [ ] Normalize Gemini response to `LLMResponse` format
  - [ ] Handle errors: rate limits, timeouts, invalid keys, safety filters
  - [ ] Unit tests with mocked Gemini SDK

### Phase 3: Create Factory
- [ ] **Task 4: Build ProviderFactory** (AC: 5, 6-9, 16-17)
  - [ ] Create `backend/providers/factory.py`
  - [ ] Implement `get_provider()` method reading `LLM_PROVIDER` env var
  - [ ] Support "groq" and "gemini" values
  - [ ] Default to "gemini" if not specified or invalid
  - [ ] Log warning for invalid provider names
  - [ ] Log active provider on startup
  - [ ] Unit tests for factory logic

### Phase 4: Integrate All Counselor Agents
- [ ] **Task 5: Refactor Health Counselor** (AC: 10)
  - [ ] Replace direct LLM SDK calls with `ProviderFactory.get_provider().generate()`
  - [ ] Maintain existing system prompts
  - [ ] Test with both Groq and Gemini

- [ ] **Task 6: Refactor Career Counselor** (AC: 10)
  - [ ] Replace direct LLM SDK calls with provider abstraction
  - [ ] Test with both providers

- [ ] **Task 7: Refactor Academic Counselor** (AC: 10)
  - [ ] Replace direct LLM SDK calls with provider abstraction
  - [ ] Test with both providers

- [ ] **Task 8: Refactor Financial Counselor** (AC: 10)
  - [ ] Replace direct LLM SDK calls with provider abstraction
  - [ ] Test with both providers

- [ ] **Task 9: Refactor Social Counselor** (AC: 10)
  - [ ] Replace direct LLM SDK calls with provider abstraction
  - [ ] Test with both providers

- [ ] **Task 10: Refactor Personal Development Counselor** (AC: 10)
  - [ ] Replace direct LLM SDK calls with provider abstraction
  - [ ] Test with both providers

### Phase 5: Framework Integration
- [ ] **Task 11: Verify PipeCat voice integration** (AC: 11)
  - [ ] Test voice sessions with Groq provider
  - [ ] Test voice sessions with Gemini provider
  - [ ] Verify Deepgram STT  Provider  Cartesia TTS flow

- [ ] **Task 12: Verify Beyond Presence avatar integration** (AC: 12)
  - [ ] Test video sessions with Groq provider
  - [ ] Test video sessions with Gemini provider
  - [ ] Verify avatar lip-sync works with both providers

- [ ] **Task 13: Verify crisis detection** (AC: 13)
  - [ ] Test crisis keyword detection with Groq
  - [ ] Test crisis keyword detection with Gemini

### Phase 6: Testing & Validation
- [ ] **Task 14: Comprehensive integration testing** (AC: 19-20)
  - [ ] Set `LLM_PROVIDER=groq`: Test all 6 counselor categories
  - [ ] Set `LLM_PROVIDER=gemini`: Test all 6 counselor categories
  - [ ] Verify responses are appropriate for each counselor type

- [ ] **Task 15: Concurrent session testing** (AC: 14, 21)
  - [ ] Simulate 10+ simultaneous counseling sessions
  - [ ] Verify no resource conflicts or deadlocks
  - [ ] Test with both Groq and Gemini

- [ ] **Task 16: Performance benchmarking** (AC: 22)
  - [ ] Measure baseline latency (direct SDK calls)
  - [ ] Measure abstraction layer latency
  - [ ] Confirm <50ms overhead

- [ ] **Task 17: Provider switching validation** (AC: 15)
  - [ ] Start system with `LLM_PROVIDER=gemini`
  - [ ] Generate responses, verify Gemini is used
  - [ ] Change to `LLM_PROVIDER=groq`, restart
  - [ ] Verify Groq is now used across all counselors

### Phase 7: Documentation
- [ ] **Task 18: Create documentation**
  - [ ] Configuration guide: How to set `LLM_PROVIDER` env var
  - [ ] Provider switching guide: Change env var + restart
  - [ ] Troubleshooting: Invalid API keys, rate limits
  - [ ] Developer guide: How to add 3rd provider (e.g., Claude)

---

## Dev Notes

### Story Context

**Existing System Integration:**
- **Integrates with:** All counselor category agents (Health, Career, Academic, Financial, Social, Personal Development) 
- **Technology:** FastAPI (Python 3.11+), PipeCat voice framework, Beyond Presence avatar system
- **Current state:** System likely uses OpenAI directly; needs abstraction to support Groq and Gemini
- **Touch points:** All counselor conversation generation, voice sessions, video sessions, crisis detection

**Simplified Architecture:** One provider for entire platform, configured via single environment variable.

### Architecture Implementation

**Simplified Architecture Pattern:**

``python
# backend/providers/config.py
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini")  # "groq" or "gemini"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# backend/providers/factory.py
class ProviderFactory:
    @staticmethod
    def get_provider() -> LLMProvider:
        """Returns single provider instance for entire system"""
        provider_name = os.getenv("LLM_PROVIDER", "gemini").lower()
        
        if provider_name == "groq":
            return GroqAdapter(api_key=os.getenv("GROQ_API_KEY"))
        elif provider_name == "gemini":
            return GeminiAdapter(api_key=os.getenv("GEMINI_API_KEY"))
        else:
            logger.warning(f"Unknown provider {provider_name}, using gemini")
            return GeminiAdapter(api_key=os.getenv("GEMINI_API_KEY"))
``

**Usage Pattern - All Counselors:**

``python
# Every counselor agent uses same pattern:
from backend.providers.factory import ProviderFactory

# Get the single provider instance for entire system
provider = ProviderFactory.get_provider()

# Generate response (same for Health, Career, Academic, etc.)
response = provider.generate(
    prompt=student_message,
    system_message=counselor_system_prompt,
    temperature=0.7,
    max_tokens=500
)
``

### Integration Points

**Framework Integration:**
- **All counselor categories** import `ProviderFactory.get_provider()` instead of direct SDK calls
- **PipeCat voice:** Provider response  Cartesia TTS  Daily.co WebRTC
- **Beyond Presence avatar:** Provider response  Avatar lip-sync  LiveKit WebRTC
- **Crisis detection:** Same provider used for analyzing conversation content

### Environment Configuration

**Environment Variables (.env file):**
``bash
# Choose ONE provider for entire system
LLM_PROVIDER=gemini  # or "groq"

# Provide both API keys (only active one is used)
GROQ_API_KEY=gsk_your_groq_key_here
GEMINI_API_KEY=your_gemini_key_here
``

### Provider Details

**Provider SDKs:**
``bash
# Install both SDKs
pip install groq google-generativeai
``

**Provider Differences to Handle:**
- **Groq:** Fast inference (low latency), uses OpenAI-compatible API format
- **Gemini:** Google's model, has safety filters, different API format
- Abstraction layer normalizes both into same response format

### Critical Integration Points

1. All counselor category routers/services
2. PipeCat bot configuration (voice)
3. Beyond Presence avatar integration (video)
4. Crisis detection keyword analyzer
5. Session logging (track which provider was used)

### Error Handling Strategy

- Both providers can hit rate limits  return `RateLimitError`
- Both can timeout  return `TimeoutError`
- Invalid API keys  return `InvalidKeyError` on first request
- Unified error handling allows counselor code to handle errors generically

### Performance Considerations

- Provider SDK connection pooling: Reuse HTTP connections where possible
- Lazy initialization: Don't instantiate provider until first request
- Concurrent requests: Ensure thread-safety for multiple counselor sessions using same provider
- Target: <50ms abstraction overhead per AC 22

### Key Constraints

- ONE provider active at a time for entire system
- Switching providers requires environment variable change + restart
- Both API keys can be present, but only configured provider is used
- All counselor agents must work identically regardless of provider choice

---

## Testing

### Testing Standards

**Test Framework:** pytest with pytest-asyncio for async tests  
**Test Location:** `packages/backend/tests/providers/`  
**Coverage Target:** 90%+ for provider adapters and factory  
**Mocking:** unittest.mock for SDK calls in unit tests

### Test Structure

``
packages/backend/tests/providers/
 test_groq_adapter.py           # Unit tests for Groq
 test_gemini_adapter.py         # Unit tests for Gemini
 test_factory.py                # Factory env var logic
 test_all_counselors_groq.py    # Integration: 6 counselors with Groq
 test_all_counselors_gemini.py  # Integration: 6 counselors with Gemini
 test_concurrent_sessions.py    # Load test
 test_provider_switching.py     # Switch between providers
``

### Integration Test Pattern

``python
# test_all_counselors_groq.py
@pytest.mark.integration
def test_all_counselors_with_groq():
    os.environ["LLM_PROVIDER"] = "groq"
    
    counselor_categories = [
        "Health", "Career", "Academic", 
        "Financial", "Social", "PersonalDevelopment"
    ]
    
    for category in counselor_categories:
        provider = ProviderFactory.get_provider()
        response = provider.generate(
            prompt="Hello, I need help",
            system_message=f"You are a {category} counselor",
            temperature=0.7,
            max_tokens=100
        )
        assert response.content
        assert response.provider_name == "groq"
``

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-20 | 1.0 | Initial story draft | Sarah (PO) |
| 2025-12-20 | 2.0 | All counselors + all frameworks integration | Sarah (PO) |
| 2025-12-20 | 3.0 | Simplified: ONE provider for entire system (Groq OR Gemini) | Sarah (PO) |

---

## Dev Agent Record

*This section will be populated by the development agent during implementation*

---

## QA Results

*This section will be populated by the QA agent after testing*

---

## Summary

**Simplified Architecture:**

 **ONE provider configuration** (`LLM_PROVIDER=groq` or `LLM_PROVIDER=gemini`)  
 **All 6 counselor agents** use the same provider  
 **Easy switching:** Change env var, restart, done  
 **Works with:** PipeCat voice, Beyond Presence video, crisis detection  
 **Foundation for future:** Later stories can add automatic failover between providers

This story provides flexibility to switch between Groq and Gemini without code changes, while keeping the architecture simple and maintainable.
